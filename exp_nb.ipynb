{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6254c05f-2094-4791-88ca-c6a459c1ddf5",
   "metadata": {},
   "source": [
    "Implementation of [**A Neural Probabilistic Language Model**](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) by Yoshua Bengio. \n",
    "<br>\n",
    "Datase used for training:- [**Tiny Shakespeare**](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c932ee-3d9b-48a2-81b2-d24bcae9d04b",
   "metadata": {},
   "source": [
    "#### Architecture (paper):\n",
    "```\n",
    "Embedding lookup → concatenate → hidden (tanh) → logits = Wx + Uh + b → softmax → cross-entropy loss.\n",
    "```\n",
    "Defaults for Tiny Shakespeare (good starting point):\n",
    "- `context_len (k) = 10`\n",
    "- `vocab_size (V) ≈ 65` (compute from text)\n",
    "- `embed_dim (m) = 32`\n",
    "- `hidden_size (H) = 128`\n",
    "- `batch_size = 128`\n",
    "- `optimizer = Adam(lr=1e-3)`\n",
    "- `epochs = 10–30`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0182b-a919-4f10-a6a3-3e541a977f97",
   "metadata": {},
   "source": [
    "### Data Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc8e27-8084-4a31-aae6-a6d183543c24",
   "metadata": {},
   "source": [
    "get dataset via API:- \n",
    "```py\n",
    "    import requests\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    text = requests.get(url).text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e8d4ba-28b1-4c1e-b7ee-901670873279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset\n",
    "text = open(\"input.txt\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c0096f-3416-4d6e-a438-04e357724110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7e51be-1657-469b-8fdc-61dd711cb385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255ea974-ed20-4681-9a75-d9a4729de5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 40001\n",
      "Vocab size: 65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "lines = text.split(\"\\n\")\n",
    "print(\"Total lines:\", len(lines))\n",
    "\n",
    "vocab = sorted(list(set(text)))\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08d8108-d1e7-46ff-a728-5f569dbe33d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Build vacabulary\n",
    "stoi = {s: i for i, s in enumerate(vocab)}\n",
    "itos = {i: s for i, s in enumerate(vocab)}\n",
    "V = len(vocab)\n",
    "itos[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd79d7f2-6adc-4e1f-a9fe-a2f05f7f3ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Encode whole text to integers\n",
    "data = [stoi[ch] for ch in text]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2dc32c-17c4-4011-8f02-e20afe702cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1003854\n",
      "testing: 111540\n"
     ]
    }
   ],
   "source": [
    "# 4. Train/Validation split\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]\n",
    "print(f\"training: {len(train_data)}\")\n",
    "print(f\"testing: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f25fb45-87c0-4f81-8c7c-503635b745c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4aad03-1707-4623-b7c5-3e2fdb9ef656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. build sliding window dataset\n",
    "\n",
    "def build_dataset(data, k):\n",
    "    X, y = [], []\n",
    "    for i in range(k, len(data)):\n",
    "        X.append(data[i-k:i])   # context\n",
    "        y.append(data[i])       # target\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = build_dataset(train_data, k)\n",
    "X_val, y_val     = build_dataset(test_data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd615bef-4976-47a5-996c-c5f5d2548829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 47, 56, 57, 58, 1, 15, 47, 58, 47],\n",
       " [47, 56, 57, 58, 1, 15, 47, 58, 47, 64],\n",
       " [56, 57, 58, 1, 15, 47, 58, 47, 64, 43],\n",
       " [57, 58, 1, 15, 47, 58, 47, 64, 43, 52],\n",
       " [58, 1, 15, 47, 58, 47, 64, 43, 52, 10]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d096dab-4c5e-464d-aa51-1af9de3a3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21687d6d-3cbf-465f-b0f8-04601e3ee9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. batching, using PyTorch\n",
    "\n",
    "def to_loader(X, y, shuffle=True, batch_size=128):\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f8ff5a-0fdc-4d54-af1e-758931067968",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = to_loader(X_train, y_train, shuffle=True)\n",
    "val_loader   = to_loader(X_val, y_val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592c4b4d-2c70-42c2-88f5-e8cf3cfa4a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10]) torch.Size([128])\n",
      "tensor([[ 0, 35, 46,  ..., 39, 50, 53],\n",
      "        [ 1, 44, 39,  ..., 57,  1, 52],\n",
      "        [ 1, 25, 39,  ...,  1, 20, 39],\n",
      "        ...,\n",
      "        [58,  1, 53,  ..., 63,  1, 40],\n",
      "        [ 1, 59, 57,  ...,  1, 47, 52],\n",
      "        [58,  0, 41,  ..., 43, 56, 44]])\n",
      "tensor([59, 53, 41, 50, 43,  1,  0, 39, 57, 52, 46, 52, 50, 59, 58, 50, 53, 53,\n",
      "        17, 39,  6,  1, 56, 43, 59,  1,  1,  1, 54, 45, 47, 58, 58, 50, 50,  6,\n",
      "        63, 46, 60, 53, 51,  0, 52, 57, 39, 43, 42, 47, 39, 59, 33, 50,  1,  1,\n",
      "         5, 57, 42, 47, 24, 47,  1, 53, 43,  1, 47, 44, 59, 57,  1, 60, 53,  8,\n",
      "        59,  1, 46, 43, 44, 17, 15,  8, 39, 56, 56, 43, 46,  1, 58, 32, 57, 44,\n",
      "        39, 46, 52, 10,  6, 53, 26,  5,  1, 46, 41,  0, 47, 50,  0, 57, 57, 43,\n",
      "        57, 13,  1, 10,  8,  1, 47,  1,  1, 57, 20,  6, 58, 39, 53, 50, 57, 56,\n",
      "        58, 43])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(xb.shape, yb.shape)\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073f2d9-258a-4b12-a86c-03f6b279bca0",
   "metadata": {},
   "source": [
    "### Forward Pass:-\n",
    "```\n",
    "    indices → embeddings → flatten → hidden(tanh) → Wx + Uh → logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ea3b1-b0d2-42b1-a156-57d7208dde22",
   "metadata": {},
   "source": [
    "Parameters (names & shapes):\n",
    "- `C` (Embedding): `nn.Embedding(V, m)` → embedding outputs `[B, k, m]`.\n",
    "- After flatten: `x = reshape([B, k*m])` → shape `[B, k*m]`.\n",
    "- `H` (hidden linear): `nn.Linear(k*m, H)` so `Hx + b_h` → `[B, H]`.\n",
    "- Activation: `h = torch.tanh(Hx + b_h)` → `[B, H]`.\n",
    "- `W` (linear from input to logits): `nn.Linear(k*m, V, bias=False)` — gives `Wx → [B, V]`.\n",
    "- `U` (linear from hidden to logits): `nn.Linear(H, V, bias=False)` — gives `Uh → [B, V]`.\n",
    "- `b` (output bias):` nn.Parameter(torch.zeros(V))` or `nn.Linear(..., V, bias=True)` combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5b6bbaf-16b7-454e-9811-3920e9d4a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init params\n",
    "k = 10 # context elngth\n",
    "V = len(vocab) # vocab size\n",
    "m = 32 # embedding dimension\n",
    "h = 128 # hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee7880f2-f426-49ba-a356-83224030b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLM(nn.Module):\n",
    "    def __init__(self, vocab_size, context_len, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparameters \n",
    "        self.V = vocab_size\n",
    "        self.k = context_len\n",
    "        self.m = embed_dim\n",
    "        self.h = hidden_dim\n",
    "\n",
    "        # ---- Layers ----\n",
    "        # Matrix C (embedding matrix)\n",
    "        self.embedding = nn.Embedding(self.V, self.m)\n",
    "\n",
    "        # Hidden layer: Hx + b\n",
    "        self.hidden_linear = nn.Linear(self.k * self.m, self.h)\n",
    "\n",
    "        # Linear shortcut Wx (no bias)\n",
    "        self.input_to_vocab = nn.Linear(self.k * self.m, self.V, bias=False)\n",
    "\n",
    "        # Nonlinear path Uh (no bias)\n",
    "        self.hidden_to_vocab = nn.Linear(self.h, self.V, bias=False)\n",
    "\n",
    "        # Output bias b\n",
    "        self.bias = nn.Parameter(torch.zeros(self.V))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, k]\n",
    "\n",
    "        # 1. Embedding lookup → [B, k, m]\n",
    "        emb = self.embedding(x)\n",
    "\n",
    "        # 2. Flatten context → [B, k*m]\n",
    "        B = emb.shape[0]\n",
    "        x_flat = emb.view(B, -1)\n",
    "\n",
    "        # 3. Hidden tanh layer\n",
    "        hidden = torch.tanh(self.hidden_linear(x_flat))\n",
    "\n",
    "        # 4. Bengio output: Wx + Uh + b\n",
    "        logits = (\n",
    "            self.input_to_vocab(x_flat)\n",
    "            + self.hidden_to_vocab(hidden)\n",
    "            + self.bias\n",
    "        )\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07770031-36f5-4fc6-a068-3cd9b265f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "model = BengioLM(vocab_size=65, context_len=10, embed_dim=32, hidden_dim=128)\n",
    "\n",
    "xb = torch.randint(0, 65, (32, 10))  # fake batch\n",
    "logits = model(xb)\n",
    "\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5e348-17f2-4213-92aa-3ed2227e490d",
   "metadata": {},
   "source": [
    "Sanity check on just one batch to verify sharpe correct, `loss` is almost `log(V)`, gradients exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce99c039-2c0b-46aa-8060-f54fd95bb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "281e9475-da99-4b7d-9a8c-cfd1a284e4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10]) torch.int64 torch.Size([128]) torch.int64\n",
      "logits shape: torch.Size([128, 65])\n",
      "tensor(4.4325, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(xb.shape, xb.dtype, yb.shape, yb.dtype)\n",
    "    \n",
    "    # forward pass\n",
    "    logits = model(xb)\n",
    "    print(f'logits shape: {logits.shape}')\n",
    "\n",
    "    loss = criterion(logits, yb)\n",
    "    print(loss)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb157cc9-4d85-4d9a-ab4b-faa9ec30b3ce",
   "metadata": {},
   "source": [
    "**One batch Overfit test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "695c0059-b6fa-469b-b943-f0e1af17f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.306519508361816\n",
      "25: 0.2483793944120407\n",
      "50: 0.034569647163152695\n",
      "75: 0.016709206625819206\n",
      "100: 0.011265597306191921\n",
      "125: 0.008346375077962875\n",
      "150: 0.0064804451540112495\n",
      "175: 0.005197469145059586\n",
      "200: 0.004273149184882641\n",
      "225: 0.0035832326393574476\n",
      "250: 0.003053407184779644\n",
      "275: 0.002636855700984597\n",
      "300: 0.002302915323525667\n",
      "325: 0.0020306732039898634\n",
      "350: 0.001805576030164957\n",
      "375: 0.0016171414172276855\n",
      "400: 0.0014576433459296823\n",
      "425: 0.0013214421924203634\n",
      "450: 0.001204045140184462\n",
      "475: 0.0011021110694855452\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "\n",
    "for step in range(500):\n",
    "    optimizer.zero_grad()  # since all model parameters are manged by Adam optimizer\n",
    "    logits = model(xb)     # forward pass\n",
    "    loss = criterion(logits, yb)\n",
    "    if step % 25 == 0: print(f\"{step}: {loss}\")  \n",
    "    loss.backward()    # backward pass (compute gradients)\n",
    "    optimizer.step()   # update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "945ed792-8c52-4114-b68d-1801f84ad2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea4ea11d-4771-4b35-9224-5c6350c3fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_ppl = math.exp(avg_train_loss)\n",
    "\n",
    "        # VALIDATION\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_ppl = math.exp(avg_val_loss)\n",
    "\n",
    "        # LOGGING\n",
    "        print(f\"Epoch {epoch:02d} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.3f} | Train PPL: {train_ppl:.2f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.3f} | Val PPL: {val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bd8297d-0f83-4030-abb9-a6bd798f3a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 1.981 | Train PPL: 7.25 | Val Loss: 1.908 | Val PPL: 6.74\n",
      "Epoch 02 | Train Loss: 1.715 | Train PPL: 5.56 | Val Loss: 1.851 | Val PPL: 6.37\n",
      "Epoch 03 | Train Loss: 1.652 | Train PPL: 5.22 | Val Loss: 1.813 | Val PPL: 6.13\n",
      "Epoch 04 | Train Loss: 1.620 | Train PPL: 5.05 | Val Loss: 1.803 | Val PPL: 6.07\n",
      "Epoch 05 | Train Loss: 1.599 | Train PPL: 4.95 | Val Loss: 1.790 | Val PPL: 5.99\n",
      "Epoch 06 | Train Loss: 1.584 | Train PPL: 4.87 | Val Loss: 1.783 | Val PPL: 5.95\n",
      "Epoch 07 | Train Loss: 1.572 | Train PPL: 4.82 | Val Loss: 1.783 | Val PPL: 5.95\n",
      "Epoch 08 | Train Loss: 1.563 | Train PPL: 4.77 | Val Loss: 1.774 | Val PPL: 5.89\n",
      "Epoch 09 | Train Loss: 1.555 | Train PPL: 4.73 | Val Loss: 1.766 | Val PPL: 5.84\n",
      "Epoch 10 | Train Loss: 1.549 | Train PPL: 4.71 | Val Loss: 1.765 | Val PPL: 5.84\n",
      "Epoch 11 | Train Loss: 1.543 | Train PPL: 4.68 | Val Loss: 1.762 | Val PPL: 5.82\n",
      "Epoch 12 | Train Loss: 1.538 | Train PPL: 4.66 | Val Loss: 1.758 | Val PPL: 5.80\n",
      "Epoch 13 | Train Loss: 1.534 | Train PPL: 4.64 | Val Loss: 1.760 | Val PPL: 5.81\n",
      "Epoch 14 | Train Loss: 1.530 | Train PPL: 4.62 | Val Loss: 1.758 | Val PPL: 5.80\n",
      "Epoch 15 | Train Loss: 1.526 | Train PPL: 4.60 | Val Loss: 1.753 | Val PPL: 5.77\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=15,\n",
    "    lr=1e-3,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5050b2fe-1422-4519-acd7-0bb50c9a8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57eba7b8-2713-4be5-a8f1-07e2c1925a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed, stoi, itos, max_new_chars=300, temperature=1.0, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    context_len = model.k  # Bengio context size\n",
    "\n",
    "    # Convert seed to indices\n",
    "    context = [stoi[c] for c in seed]\n",
    "\n",
    "    # Pad if seed shorter than context\n",
    "    if len(context) < context_len:\n",
    "        context = [0] * (context_len - len(context)) + context\n",
    "\n",
    "    context = context[-context_len:]  # keep last k\n",
    "\n",
    "    generated = seed\n",
    "\n",
    "    for _ in range(max_new_chars):\n",
    "        x = torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0)  # [1, k]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)  # [1, V]\n",
    "            logits = logits[0] / temperature  # temperature scaling\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample next char\n",
    "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # Convert back to char\n",
    "        next_char = itos[next_idx]\n",
    "        generated += next_char\n",
    "\n",
    "        # Slide context window\n",
    "        context = context[1:] + [next_idx]\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "453545ee-3770-494c-98ba-7614ae87f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING:\n",
      "Clife, Paris, to his bear that a\n",
      "Will'd great Ay, and\n",
      "have thou detion. True and the way.\n",
      "\n",
      "ISABELLA:\n",
      "The should witching.\n",
      "\n",
      "SICINIUS:\n",
      "He'll diese,\n",
      "Were had arms:\n",
      "When that but servise?\n",
      "\n",
      "DERBY:\n",
      "Why, soal me, on more.\n",
      "\n",
      "ROMEO:\n",
      "So the other?\n",
      "\n",
      "BENVOLIO:\n",
      "Howsself of his brother's not but that did the like me what should not a foint of all this natullys he old and stain in a pointly the goldins,\n",
      "And here to slege curded holy, my lord.\n",
      "\n",
      "KING RICHARD III:\n",
      "Now now, hon that you. Tranks,\n",
      "More than water'd i\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\n",
    "    model,\n",
    "    seed=\"KING:\\n\",\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    max_new_chars=500,\n",
    "    temperature=0.8,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c6ad2-56ae-4d4f-9a14-05a9fe64732c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
